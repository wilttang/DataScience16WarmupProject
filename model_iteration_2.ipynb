{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    We can start the next iteration by using techniques in improving submission category \n",
    "    \n",
    "    The end goal will be to combine different algorithms. We have already created 2: A logistical regression and random trees model. \n",
    "    First we will attempt to improve the logistical regression model by training it by doing better cross validation splits or changing the scoring method(log(loss)) \n",
    "    In addtion we will add combine the age and gender category to create a young and old male/female to see if this new category will improve our system. We can try extracting crew status(included into title).\n",
    "   \n",
    "    After reaching a good point we will combine the 2 to see if we can improve our score\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p>We can start off by prepping our data for both test and train. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Col           2\n",
      "Major         2\n",
      "Mlle          2\n",
      "Countess      1\n",
      "Ms            1\n",
      "Lady          1\n",
      "Jonkheer      1\n",
      "Don           1\n",
      "Mme           1\n",
      "Capt          1\n",
      "Sir           1\n",
      "Name: Name, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wilson/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "import re\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "titanic = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Clean and Prep train Data\n",
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "#add Title\n",
    "# A function to get the title from a name.\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pd.value_counts(titles))\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "# Add in the title column.\n",
    "titanic[\"Title\"] = titles\n",
    "\n",
    "titanic_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#Clean and Prep test data\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n",
    "#add title \n",
    "# Add Title to the test data\n",
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# Map each title to an integer.  Some titles are very rare, and are compressed into the same codes as other titles.\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "# Add in the title column.\n",
    "titanic_test[\"Title\"] = titles\n",
    "\n",
    "titanic.describe()\n",
    "\n",
    "#method taken from datquest\n",
    "#accuracy methods\n",
    "def calculate_accuracy(predictions_data):\n",
    "    # Map predictions to outcomes (only possible outcomes are 1 and 0)\n",
    "    predictions_data[predictions_data > .5] = 1\n",
    "    predictions_data[predictions_data <=.5] = 0\n",
    "    accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "    print accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> We will be seeing if we can improve our model by combining and reclassifying features. From explorations we notice that Age and Gender are related but the relationship is hard to capture with only those two so we can combine and split into 4 categories: young male, young female, adult male, adult female.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "5              6         0       3   \n",
      "6              7         0       1   \n",
      "7              8         0       3   \n",
      "8              9         1       3   \n",
      "9             10         1       2   \n",
      "10            11         1       3   \n",
      "11            12         1       1   \n",
      "12            13         0       3   \n",
      "13            14         0       3   \n",
      "14            15         0       3   \n",
      "15            16         1       2   \n",
      "16            17         0       3   \n",
      "17            18         1       2   \n",
      "18            19         0       3   \n",
      "19            20         1       3   \n",
      "20            21         0       2   \n",
      "21            22         1       2   \n",
      "22            23         1       3   \n",
      "23            24         1       1   \n",
      "24            25         0       3   \n",
      "25            26         1       3   \n",
      "26            27         0       3   \n",
      "27            28         0       1   \n",
      "28            29         1       3   \n",
      "29            30         0       3   \n",
      "..           ...       ...     ...   \n",
      "861          862         0       2   \n",
      "862          863         1       1   \n",
      "863          864         0       3   \n",
      "864          865         0       2   \n",
      "865          866         1       2   \n",
      "866          867         1       2   \n",
      "867          868         0       1   \n",
      "868          869         0       3   \n",
      "869          870         1       3   \n",
      "870          871         0       3   \n",
      "871          872         1       1   \n",
      "872          873         0       1   \n",
      "873          874         0       3   \n",
      "874          875         1       2   \n",
      "875          876         1       3   \n",
      "876          877         0       3   \n",
      "877          878         0       3   \n",
      "878          879         0       3   \n",
      "879          880         1       1   \n",
      "880          881         1       2   \n",
      "881          882         0       3   \n",
      "882          883         0       3   \n",
      "883          884         0       2   \n",
      "884          885         0       3   \n",
      "885          886         0       3   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name Sex  Age  SibSp  Parch  \\\n",
      "0                              Braund, Mr. Owen Harris   0   22      1      0   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...   1   38      1      0   \n",
      "2                               Heikkinen, Miss. Laina   1   26      0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)   1   35      1      0   \n",
      "4                             Allen, Mr. William Henry   0   35      0      0   \n",
      "5                                     Moran, Mr. James   0   28      0      0   \n",
      "6                              McCarthy, Mr. Timothy J   0   54      0      0   \n",
      "7                       Palsson, Master. Gosta Leonard   0    2      3      1   \n",
      "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   1   27      0      2   \n",
      "9                  Nasser, Mrs. Nicholas (Adele Achem)   1   14      1      0   \n",
      "10                     Sandstrom, Miss. Marguerite Rut   1    4      1      1   \n",
      "11                            Bonnell, Miss. Elizabeth   1   58      0      0   \n",
      "12                      Saundercock, Mr. William Henry   0   20      0      0   \n",
      "13                         Andersson, Mr. Anders Johan   0   39      1      5   \n",
      "14                Vestrom, Miss. Hulda Amanda Adolfina   1   14      0      0   \n",
      "15                    Hewlett, Mrs. (Mary D Kingcome)    1   55      0      0   \n",
      "16                                Rice, Master. Eugene   0    2      4      1   \n",
      "17                        Williams, Mr. Charles Eugene   0   28      0      0   \n",
      "18   Vander Planke, Mrs. Julius (Emelia Maria Vande...   1   31      1      0   \n",
      "19                             Masselmani, Mrs. Fatima   1   28      0      0   \n",
      "20                                Fynney, Mr. Joseph J   0   35      0      0   \n",
      "21                               Beesley, Mr. Lawrence   0   34      0      0   \n",
      "22                         McGowan, Miss. Anna \"Annie\"   1   15      0      0   \n",
      "23                        Sloper, Mr. William Thompson   0   28      0      0   \n",
      "24                       Palsson, Miss. Torborg Danira   1    8      3      1   \n",
      "25   Asplund, Mrs. Carl Oscar (Selma Augusta Emilia...   1   38      1      5   \n",
      "26                             Emir, Mr. Farred Chehab   0   28      0      0   \n",
      "27                      Fortune, Mr. Charles Alexander   0   19      3      2   \n",
      "28                       O'Dwyer, Miss. Ellen \"Nellie\"   1   28      0      0   \n",
      "29                                 Todoroff, Mr. Lalio   0   28      0      0   \n",
      "..                                                 ...  ..  ...    ...    ...   \n",
      "861                        Giles, Mr. Frederick Edward   0   21      1      0   \n",
      "862  Swift, Mrs. Frederick Joel (Margaret Welles Ba...   1   48      0      0   \n",
      "863                  Sage, Miss. Dorothy Edith \"Dolly\"   1   28      8      2   \n",
      "864                             Gill, Mr. John William   0   24      0      0   \n",
      "865                           Bystrom, Mrs. (Karolina)   1   42      0      0   \n",
      "866                       Duran y More, Miss. Asuncion   1   27      1      0   \n",
      "867               Roebling, Mr. Washington Augustus II   0   31      0      0   \n",
      "868                        van Melkebeke, Mr. Philemon   0   28      0      0   \n",
      "869                    Johnson, Master. Harold Theodor   0    4      1      1   \n",
      "870                                  Balkic, Mr. Cerin   0   26      0      0   \n",
      "871   Beckwith, Mrs. Richard Leonard (Sallie Monypeny)   1   47      1      1   \n",
      "872                           Carlsson, Mr. Frans Olof   0   33      0      0   \n",
      "873                        Vander Cruyssen, Mr. Victor   0   47      0      0   \n",
      "874              Abelson, Mrs. Samuel (Hannah Wizosky)   1   28      1      0   \n",
      "875                   Najib, Miss. Adele Kiamie \"Jane\"   1   15      0      0   \n",
      "876                      Gustafsson, Mr. Alfred Ossian   0   20      0      0   \n",
      "877                               Petroff, Mr. Nedelio   0   19      0      0   \n",
      "878                                 Laleff, Mr. Kristo   0   28      0      0   \n",
      "879      Potter, Mrs. Thomas Jr (Lily Alexenia Wilson)   1   56      0      1   \n",
      "880       Shelley, Mrs. William (Imanita Parrish Hall)   1   25      0      1   \n",
      "881                                 Markun, Mr. Johann   0   33      0      0   \n",
      "882                       Dahlberg, Miss. Gerda Ulrika   1   22      0      0   \n",
      "883                      Banfield, Mr. Frederick James   0   28      0      0   \n",
      "884                             Sutehall, Mr. Henry Jr   0   25      0      0   \n",
      "885               Rice, Mrs. William (Margaret Norton)   1   39      0      5   \n",
      "886                              Montvila, Rev. Juozas   0   27      0      0   \n",
      "887                       Graham, Miss. Margaret Edith   1   19      0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"   1   28      1      2   \n",
      "889                              Behr, Mr. Karl Howell   0   26      0      0   \n",
      "890                                Dooley, Mr. Patrick   0   32      0      0   \n",
      "\n",
      "               Ticket      Fare        Cabin Embarked Title  Young_M  Adult_M  \\\n",
      "0           A/5 21171    7.2500          NaN        0     1        0        1   \n",
      "1            PC 17599   71.2833          C85        1     3        0        0   \n",
      "2    STON/O2. 3101282    7.9250          NaN        0     2        0        0   \n",
      "3              113803   53.1000         C123        0     3        0        0   \n",
      "4              373450    8.0500          NaN        0     1        0        1   \n",
      "5              330877    8.4583          NaN        2     1        0        1   \n",
      "6               17463   51.8625          E46        0     1        0        1   \n",
      "7              349909   21.0750          NaN        0     4        1        0   \n",
      "8              347742   11.1333          NaN        0     3        0        0   \n",
      "9              237736   30.0708          NaN        1     3        0        0   \n",
      "10            PP 9549   16.7000           G6        0     2        0        0   \n",
      "11             113783   26.5500         C103        0     2        0        0   \n",
      "12          A/5. 2151    8.0500          NaN        0     1        0        1   \n",
      "13             347082   31.2750          NaN        0     1        0        1   \n",
      "14             350406    7.8542          NaN        0     2        0        0   \n",
      "15             248706   16.0000          NaN        0     3        0        0   \n",
      "16             382652   29.1250          NaN        2     4        1        0   \n",
      "17             244373   13.0000          NaN        0     1        0        1   \n",
      "18             345763   18.0000          NaN        0     3        0        0   \n",
      "19               2649    7.2250          NaN        1     3        0        0   \n",
      "20             239865   26.0000          NaN        0     1        0        1   \n",
      "21             248698   13.0000          D56        0     1        0        1   \n",
      "22             330923    8.0292          NaN        2     2        0        0   \n",
      "23             113788   35.5000           A6        0     1        0        1   \n",
      "24             349909   21.0750          NaN        0     2        0        0   \n",
      "25             347077   31.3875          NaN        0     3        0        0   \n",
      "26               2631    7.2250          NaN        1     1        0        1   \n",
      "27              19950  263.0000  C23 C25 C27        0     1        0        1   \n",
      "28             330959    7.8792          NaN        2     2        0        0   \n",
      "29             349216    7.8958          NaN        0     1        0        1   \n",
      "..                ...       ...          ...      ...   ...      ...      ...   \n",
      "861             28134   11.5000          NaN        0     1        0        1   \n",
      "862             17466   25.9292          D17        0     3        0        0   \n",
      "863          CA. 2343   69.5500          NaN        0     2        0        0   \n",
      "864            233866   13.0000          NaN        0     1        0        1   \n",
      "865            236852   13.0000          NaN        0     3        0        0   \n",
      "866     SC/PARIS 2149   13.8583          NaN        1     2        0        0   \n",
      "867          PC 17590   50.4958          A24        0     1        0        1   \n",
      "868            345777    9.5000          NaN        0     1        0        1   \n",
      "869            347742   11.1333          NaN        0     4        1        0   \n",
      "870            349248    7.8958          NaN        0     1        0        1   \n",
      "871             11751   52.5542          D35        0     3        0        0   \n",
      "872               695    5.0000  B51 B53 B55        0     1        0        1   \n",
      "873            345765    9.0000          NaN        0     1        0        1   \n",
      "874         P/PP 3381   24.0000          NaN        1     3        0        0   \n",
      "875              2667    7.2250          NaN        1     2        0        0   \n",
      "876              7534    9.8458          NaN        0     1        0        1   \n",
      "877            349212    7.8958          NaN        0     1        0        1   \n",
      "878            349217    7.8958          NaN        0     1        0        1   \n",
      "879             11767   83.1583          C50        1     3        0        0   \n",
      "880            230433   26.0000          NaN        0     3        0        0   \n",
      "881            349257    7.8958          NaN        0     1        0        1   \n",
      "882              7552   10.5167          NaN        0     2        0        0   \n",
      "883  C.A./SOTON 34068   10.5000          NaN        0     1        0        1   \n",
      "884   SOTON/OQ 392076    7.0500          NaN        0     1        0        1   \n",
      "885            382652   29.1250          NaN        2     3        0        0   \n",
      "886            211536   13.0000          NaN        0     6        0        1   \n",
      "887            112053   30.0000          B42        0     2        0        0   \n",
      "888        W./C. 6607   23.4500          NaN        0     2        0        0   \n",
      "889            111369   30.0000         C148        1     1        0        1   \n",
      "890            370376    7.7500          NaN        2     1        0        1   \n",
      "\n",
      "     Young_F  Adult_F  \n",
      "0          0        0  \n",
      "1          0        1  \n",
      "2          0        1  \n",
      "3          0        1  \n",
      "4          0        0  \n",
      "5          0        0  \n",
      "6          0        0  \n",
      "7          0        0  \n",
      "8          0        1  \n",
      "9          1        0  \n",
      "10         1        0  \n",
      "11         0        1  \n",
      "12         0        0  \n",
      "13         0        0  \n",
      "14         1        0  \n",
      "15         0        1  \n",
      "16         0        0  \n",
      "17         0        0  \n",
      "18         0        1  \n",
      "19         0        1  \n",
      "20         0        0  \n",
      "21         0        0  \n",
      "22         1        0  \n",
      "23         0        0  \n",
      "24         1        0  \n",
      "25         0        1  \n",
      "26         0        0  \n",
      "27         0        0  \n",
      "28         0        1  \n",
      "29         0        0  \n",
      "..       ...      ...  \n",
      "861        0        0  \n",
      "862        0        1  \n",
      "863        0        1  \n",
      "864        0        0  \n",
      "865        0        1  \n",
      "866        0        1  \n",
      "867        0        0  \n",
      "868        0        0  \n",
      "869        0        0  \n",
      "870        0        0  \n",
      "871        0        1  \n",
      "872        0        0  \n",
      "873        0        0  \n",
      "874        0        1  \n",
      "875        1        0  \n",
      "876        0        0  \n",
      "877        0        0  \n",
      "878        0        0  \n",
      "879        0        1  \n",
      "880        0        1  \n",
      "881        0        0  \n",
      "882        0        1  \n",
      "883        0        0  \n",
      "884        0        0  \n",
      "885        0        1  \n",
      "886        0        0  \n",
      "887        0        1  \n",
      "888        0        1  \n",
      "889        0        0  \n",
      "890        0        0  \n",
      "\n",
      "[891 rows x 17 columns]\n",
      "     PassengerId  Pclass                                               Name  \\\n",
      "0            892       3                                   Kelly, Mr. James   \n",
      "1            893       3                   Wilkes, Mrs. James (Ellen Needs)   \n",
      "2            894       2                          Myles, Mr. Thomas Francis   \n",
      "3            895       3                                   Wirz, Mr. Albert   \n",
      "4            896       3       Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
      "5            897       3                         Svensson, Mr. Johan Cervin   \n",
      "6            898       3                               Connolly, Miss. Kate   \n",
      "7            899       2                       Caldwell, Mr. Albert Francis   \n",
      "8            900       3          Abrahim, Mrs. Joseph (Sophie Halaut Easu)   \n",
      "9            901       3                            Davies, Mr. John Samuel   \n",
      "10           902       3                                   Ilieff, Mr. Ylio   \n",
      "11           903       1                         Jones, Mr. Charles Cresson   \n",
      "12           904       1      Snyder, Mrs. John Pillsbury (Nelle Stevenson)   \n",
      "13           905       2                               Howard, Mr. Benjamin   \n",
      "14           906       1  Chaffee, Mrs. Herbert Fuller (Carrie Constance...   \n",
      "15           907       2      del Carlo, Mrs. Sebastiano (Argenia Genovesi)   \n",
      "16           908       2                                  Keane, Mr. Daniel   \n",
      "17           909       3                                  Assaf, Mr. Gerios   \n",
      "18           910       3                       Ilmakangas, Miss. Ida Livija   \n",
      "19           911       3              Assaf Khalil, Mrs. Mariana (Miriam\")\"   \n",
      "20           912       1                             Rothschild, Mr. Martin   \n",
      "21           913       3                          Olsen, Master. Artur Karl   \n",
      "22           914       1               Flegenheim, Mrs. Alfred (Antoinette)   \n",
      "23           915       1                    Williams, Mr. Richard Norris II   \n",
      "24           916       1    Ryerson, Mrs. Arthur Larned (Emily Maria Borie)   \n",
      "25           917       3                            Robins, Mr. Alexander A   \n",
      "26           918       1                       Ostby, Miss. Helene Ragnhild   \n",
      "27           919       3                                  Daher, Mr. Shedid   \n",
      "28           920       1                            Brady, Mr. John Bertram   \n",
      "29           921       3                                  Samaan, Mr. Elias   \n",
      "..           ...     ...                                                ...   \n",
      "388         1280       3                               Canavan, Mr. Patrick   \n",
      "389         1281       3                        Palsson, Master. Paul Folke   \n",
      "390         1282       1                         Payne, Mr. Vivian Ponsonby   \n",
      "391         1283       1     Lines, Mrs. Ernest H (Elizabeth Lindsey James)   \n",
      "392         1284       3                      Abbott, Master. Eugene Joseph   \n",
      "393         1285       2                               Gilbert, Mr. William   \n",
      "394         1286       3                           Kink-Heilmann, Mr. Anton   \n",
      "395         1287       1     Smith, Mrs. Lucien Philip (Mary Eloise Hughes)   \n",
      "396         1288       3                               Colbert, Mr. Patrick   \n",
      "397         1289       1  Frolicher-Stehli, Mrs. Maxmillian (Margaretha ...   \n",
      "398         1290       3                     Larsson-Rondberg, Mr. Edvard A   \n",
      "399         1291       3                           Conlon, Mr. Thomas Henry   \n",
      "400         1292       1                            Bonnell, Miss. Caroline   \n",
      "401         1293       2                                    Gale, Mr. Harry   \n",
      "402         1294       1                     Gibson, Miss. Dorothy Winifred   \n",
      "403         1295       1                             Carrau, Mr. Jose Pedro   \n",
      "404         1296       1                       Frauenthal, Mr. Isaac Gerald   \n",
      "405         1297       2       Nourney, Mr. Alfred (Baron von Drachstedt\")\"   \n",
      "406         1298       2                          Ware, Mr. William Jeffery   \n",
      "407         1299       1                         Widener, Mr. George Dunton   \n",
      "408         1300       3                    Riordan, Miss. Johanna Hannah\"\"   \n",
      "409         1301       3                          Peacock, Miss. Treasteall   \n",
      "410         1302       3                             Naughton, Miss. Hannah   \n",
      "411         1303       1    Minahan, Mrs. William Edward (Lillian E Thorpe)   \n",
      "412         1304       3                     Henriksson, Miss. Jenny Lovisa   \n",
      "413         1305       3                                 Spector, Mr. Woolf   \n",
      "414         1306       1                       Oliva y Ocana, Dona. Fermina   \n",
      "415         1307       3                       Saether, Mr. Simon Sivertsen   \n",
      "416         1308       3                                Ware, Mr. Frederick   \n",
      "417         1309       3                           Peter, Master. Michael J   \n",
      "\n",
      "    Sex   Age  SibSp  Parch              Ticket      Fare            Cabin  \\\n",
      "0     0  34.5      0      0              330911    7.8292              NaN   \n",
      "1     1  47.0      1      0              363272    7.0000              NaN   \n",
      "2     0  62.0      0      0              240276    9.6875              NaN   \n",
      "3     0  27.0      0      0              315154    8.6625              NaN   \n",
      "4     1  22.0      1      1             3101298   12.2875              NaN   \n",
      "5     0  14.0      0      0                7538    9.2250              NaN   \n",
      "6     1  30.0      0      0              330972    7.6292              NaN   \n",
      "7     0  26.0      1      1              248738   29.0000              NaN   \n",
      "8     1  18.0      0      0                2657    7.2292              NaN   \n",
      "9     0  21.0      2      0           A/4 48871   24.1500              NaN   \n",
      "10    0  28.0      0      0              349220    7.8958              NaN   \n",
      "11    0  46.0      0      0                 694   26.0000              NaN   \n",
      "12    1  23.0      1      0               21228   82.2667              B45   \n",
      "13    0  63.0      1      0               24065   26.0000              NaN   \n",
      "14    1  47.0      1      0         W.E.P. 5734   61.1750              E31   \n",
      "15    1  24.0      1      0       SC/PARIS 2167   27.7208              NaN   \n",
      "16    0  35.0      0      0              233734   12.3500              NaN   \n",
      "17    0  21.0      0      0                2692    7.2250              NaN   \n",
      "18    1  27.0      1      0    STON/O2. 3101270    7.9250              NaN   \n",
      "19    1  45.0      0      0                2696    7.2250              NaN   \n",
      "20    0  55.0      1      0            PC 17603   59.4000              NaN   \n",
      "21    0   9.0      0      1             C 17368    3.1708              NaN   \n",
      "22    1  28.0      0      0            PC 17598   31.6833              NaN   \n",
      "23    0  21.0      0      1            PC 17597   61.3792              NaN   \n",
      "24    1  48.0      1      3            PC 17608  262.3750  B57 B59 B63 B66   \n",
      "25    0  50.0      1      0           A/5. 3337   14.5000              NaN   \n",
      "26    1  22.0      0      1              113509   61.9792              B36   \n",
      "27    0  22.5      0      0                2698    7.2250              NaN   \n",
      "28    0  41.0      0      0              113054   30.5000              A21   \n",
      "29    0  28.0      2      0                2662   21.6792              NaN   \n",
      "..   ..   ...    ...    ...                 ...       ...              ...   \n",
      "388   0  21.0      0      0              364858    7.7500              NaN   \n",
      "389   0   6.0      3      1              349909   21.0750              NaN   \n",
      "390   0  23.0      0      0               12749   93.5000              B24   \n",
      "391   1  51.0      0      1            PC 17592   39.4000              D28   \n",
      "392   0  13.0      0      2           C.A. 2673   20.2500              NaN   \n",
      "393   0  47.0      0      0          C.A. 30769   10.5000              NaN   \n",
      "394   0  29.0      3      1              315153   22.0250              NaN   \n",
      "395   1  18.0      1      0               13695   60.0000              C31   \n",
      "396   0  24.0      0      0              371109    7.2500              NaN   \n",
      "397   1  48.0      1      1               13567   79.2000              B41   \n",
      "398   0  22.0      0      0              347065    7.7750              NaN   \n",
      "399   0  31.0      0      0               21332    7.7333              NaN   \n",
      "400   1  30.0      0      0               36928  164.8667               C7   \n",
      "401   0  38.0      1      0               28664   21.0000              NaN   \n",
      "402   1  22.0      0      1              112378   59.4000              NaN   \n",
      "403   0  17.0      0      0              113059   47.1000              NaN   \n",
      "404   0  43.0      1      0               17765   27.7208              D40   \n",
      "405   0  20.0      0      0       SC/PARIS 2166   13.8625              D38   \n",
      "406   0  23.0      1      0               28666   10.5000              NaN   \n",
      "407   0  50.0      1      1              113503  211.5000              C80   \n",
      "408   1  28.0      0      0              334915    7.7208              NaN   \n",
      "409   1   3.0      1      1  SOTON/O.Q. 3101315   13.7750              NaN   \n",
      "410   1  28.0      0      0              365237    7.7500              NaN   \n",
      "411   1  37.0      1      0               19928   90.0000              C78   \n",
      "412   1  28.0      0      0              347086    7.7750              NaN   \n",
      "413   0  28.0      0      0           A.5. 3236    8.0500              NaN   \n",
      "414   1  39.0      0      0            PC 17758  108.9000             C105   \n",
      "415   0  38.5      0      0  SOTON/O.Q. 3101262    7.2500              NaN   \n",
      "416   0  28.0      0      0              359309    8.0500              NaN   \n",
      "417   0  28.0      1      1                2668   22.3583              NaN   \n",
      "\n",
      "    Embarked Title  Young_M  Adult_M  Young_F  Adult_F  \n",
      "0          2     1        0        1        0        0  \n",
      "1          0     3        0        0        0        1  \n",
      "2          2     1        0        1        0        0  \n",
      "3          0     1        0        1        0        0  \n",
      "4          0     3        0        0        0        1  \n",
      "5          0     1        1        0        0        0  \n",
      "6          2     2        0        0        0        1  \n",
      "7          0     1        0        1        0        0  \n",
      "8          1     3        0        0        0        1  \n",
      "9          0     1        0        1        0        0  \n",
      "10         0     1        0        1        0        0  \n",
      "11         0     1        0        1        0        0  \n",
      "12         0     3        0        0        0        1  \n",
      "13         0     1        0        1        0        0  \n",
      "14         0     3        0        0        0        1  \n",
      "15         1     3        0        0        0        1  \n",
      "16         2     1        0        1        0        0  \n",
      "17         1     1        0        1        0        0  \n",
      "18         0     2        0        0        0        1  \n",
      "19         1     3        0        0        0        1  \n",
      "20         1     1        0        1        0        0  \n",
      "21         0     4        1        0        0        0  \n",
      "22         0     3        0        0        0        1  \n",
      "23         1     1        0        1        0        0  \n",
      "24         1     3        0        0        0        1  \n",
      "25         0     1        0        1        0        0  \n",
      "26         1     2        0        0        0        1  \n",
      "27         1     1        0        1        0        0  \n",
      "28         0     1        0        1        0        0  \n",
      "29         1     1        0        1        0        0  \n",
      "..       ...   ...      ...      ...      ...      ...  \n",
      "388        2     1        0        1        0        0  \n",
      "389        0     4        1        0        0        0  \n",
      "390        0     1        0        1        0        0  \n",
      "391        0     3        0        0        0        1  \n",
      "392        0     4        1        0        0        0  \n",
      "393        0     1        0        1        0        0  \n",
      "394        0     1        0        1        0        0  \n",
      "395        0     3        0        0        0        1  \n",
      "396        2     1        0        1        0        0  \n",
      "397        1     3        0        0        0        1  \n",
      "398        0     1        0        1        0        0  \n",
      "399        2     1        0        1        0        0  \n",
      "400        0     2        0        0        0        1  \n",
      "401        0     1        0        1        0        0  \n",
      "402        1     2        0        0        0        1  \n",
      "403        0     1        1        0        0        0  \n",
      "404        1     1        0        1        0        0  \n",
      "405        1     1        0        1        0        0  \n",
      "406        0     1        0        1        0        0  \n",
      "407        1     1        0        1        0        0  \n",
      "408        2     2        0        0        0        1  \n",
      "409        0     2        0        0        1        0  \n",
      "410        2     2        0        0        0        1  \n",
      "411        2     3        0        0        0        1  \n",
      "412        0     2        0        0        0        1  \n",
      "413        0     1        0        1        0        0  \n",
      "414        1    10        0        0        0        1  \n",
      "415        0     1        0        1        0        0  \n",
      "416        0     1        0        1        0        0  \n",
      "417        1     4        0        1        0        0  \n",
      "\n",
      "[418 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "#create the 4 new categories\n",
    "# 0 = false\n",
    "# 1 = true\n",
    "titanic[\"Young_M\"] = titanic[\"Age\"]\n",
    "titanic[\"Adult_M\"] = titanic[\"Age\"] \n",
    "titanic[\"Young_F\"] = titanic[\"Age\"]\n",
    "titanic[\"Adult_F\"] = titanic[\"Age\"] \n",
    "titanic.loc[titanic.Young_M.notnull(), 'Young_M'] = 0\n",
    "titanic.loc[titanic.Adult_M.notnull(), 'Adult_M'] = 0\n",
    "titanic.loc[titanic.Young_F.notnull(), 'Young_F'] = 0\n",
    "titanic.loc[titanic.Adult_F.notnull(), 'Adult_F'] = 0\n",
    "titanic.loc[(titanic.Age < 18) & (titanic.Sex == 0), 'Young_M'] = 1\n",
    "titanic.loc[(titanic.Age >= 18) & (titanic.Sex == 0), 'Adult_M'] = 1\n",
    "titanic.loc[(titanic.Age < 18) & (titanic.Sex == 1), 'Young_F'] = 1\n",
    "titanic.loc[(titanic.Age >= 18) & (titanic.Sex == 1), 'Adult_F'] = 1\n",
    "#sanity check that there are only 1 per row for each of the categories\n",
    "print titanic\n",
    "\n",
    "#now we do the same to titanic test\n",
    "titanic_test[\"Young_M\"] = titanic_test[\"Age\"]\n",
    "titanic_test[\"Adult_M\"] = titanic_test[\"Age\"] \n",
    "titanic_test[\"Young_F\"] = titanic_test[\"Age\"]\n",
    "titanic_test[\"Adult_F\"] = titanic_test[\"Age\"] \n",
    "titanic_test.loc[titanic_test.Young_M.notnull(), 'Young_M'] = 0\n",
    "titanic_test.loc[titanic_test.Adult_M.notnull(), 'Adult_M'] = 0\n",
    "titanic_test.loc[titanic_test.Young_F.notnull(), 'Young_F'] = 0\n",
    "titanic_test.loc[titanic_test.Adult_F.notnull(), 'Adult_F'] = 0\n",
    "titanic_test.loc[(titanic_test.Age < 18) & (titanic_test.Sex == 0), 'Young_M'] = 1\n",
    "titanic_test.loc[(titanic_test.Age >= 18) & (titanic_test.Sex == 0), 'Adult_M'] = 1\n",
    "titanic_test.loc[(titanic_test.Age < 18) & (titanic_test.Sex == 1), 'Young_F'] = 1\n",
    "titanic_test.loc[(titanic_test.Age >= 18) & (titanic_test.Sex == 1), 'Adult_F'] = 1\n",
    "#sanity check that there are only 1 per row for each of the categories\n",
    "print titanic_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Now we can try out these new categories and remove the old categories to see if it improves either of our models! </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.361582    0.523008   \n",
      "std     257.353842    0.486592    0.836071   13.019697    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   22.000000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   35.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare     Young_M     Adult_M     Young_F     Adult_F  \n",
      "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000  \n",
      "mean     0.381594   32.204208    0.065095    0.582492    0.061728    0.290685  \n",
      "std      0.806057   49.693429    0.246833    0.493425    0.240797    0.454333  \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
      "25%      0.000000    7.910400    0.000000    0.000000    0.000000    0.000000  \n",
      "50%      0.000000   14.454200    0.000000    1.000000    0.000000    0.000000  \n",
      "75%      0.000000   31.000000    0.000000    1.000000    0.000000    1.000000  \n",
      "max      6.000000  512.329200    1.000000    1.000000    1.000000    1.000000  \n",
      "0.806958473625\n",
      "0.804713804714\n",
      "0.811447811448\n"
     ]
    }
   ],
   "source": [
    "#Trying out Logistical Regression Model \n",
    "print titanic.describe()\n",
    "predictors = [\"Pclass\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Title\",\"Young_M\",\"Adult_M\",\"Young_F\",\"Adult_F\"]\n",
    "# Initialize the algorithm class\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print scores.mean()\n",
    "\n",
    "predictors = [\"Pclass\",\"Age\", \"Sex\", \"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Title\"]\n",
    "# Initialize the algorithm class\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print scores.mean()\n",
    "\n",
    "predictors = [\"Pclass\",\"Age\", \"Sex\", \"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Title\",\"Young_M\",\"Adult_M\",\"Young_F\",\"Adult_F\"]\n",
    "# Initialize the algorithm class\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> \n",
    "    From the three test we see that adding the 4 categories to the logistical regression improves the model the most.\n",
    "    Now we will try what it does to our previous version of the trees\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810325476992\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEpCAYAAABYyHNYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqt7BAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f99dc23cd90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"Title\",\"Young_M\",\"Adult_M\",\"Young_F\",\"Adult_F\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print(scores.mean())\n",
    "#method taken from datquest\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores.  See how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> From the histogram we can see the most effective trees to use. Now we will try different permutations of the best categories to determine the best tree algorithm(we found the optimal amount of categories to be 5 from before so we'll start from there)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 categories old 0.82379349046\n",
      "5 categories with new max 5 0.781144781145\n",
      "6 categories 0.826038159371\n",
      "7 categories 0.818181818182\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Embarked\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=10, min_samples_leaf= 5)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('5 categories old ' + str(scores.mean()))\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Adult_F\", \"Adult_M\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=10, min_samples_leaf= 5)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('5 categories with new max 5 ' + str(scores.mean()))\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Embarked\", \"Title\",\"Adult_M\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=10, min_samples_leaf= 5)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('6 categories ' + str(scores.mean()))\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Embarked\", \"Title\" ,\"Adult_M\",\"Adult_F\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=10, min_samples_leaf= 5)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('7 categories ' + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> from these results we see that adding these new categories on top of the already existing sex seems to not do much. However removing Fare and Embarked made the score significantly worse which probably has something to do with way trees work.\n",
    "\n",
    "Next we can try out taking out sex and using combinations of the 4 new categories in its stead\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All categories 0.82379349046\n",
      "5 categories: class,embarked,adult_f,adult_m,title 0.812570145903\n",
      "6 categories: class,embarked,fare,adult_f,adult_m,title 0.828282828283\n",
      "6 categories: class,embarked,fare,adult_f,adult_m,title 0.814814814815\n",
      "7 categories: class,embarked,fare,adult_f,adult_m,title,sex 0.818181818182\n",
      "6 categories: class,embarked,fare,adult_f,adult_m,title 0.820426487093\n"
     ]
    }
   ],
   "source": [
    "#new test\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Embarked\", \"Title\", \"Young_F\", \"Adult_M\",\"Young_M\",\"Adult_F\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=18, min_samples_leaf= 9)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('All categories ' + str(scores.mean()))\n",
    "\n",
    "#new test\n",
    "predictors = [\"Pclass\", \"Embarked\", \"Adult_F\", \"Adult_M\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=10, min_samples_leaf= 5)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('5 categories: class,embarked,adult_f,adult_m,title ' + str(scores.mean()))\n",
    "\n",
    "#new test\n",
    "predictors = [\"Pclass\", \"Embarked\", \"Fare\", \"Adult_F\", \"Adult_M\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=12, min_samples_leaf= 6)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('6 categories: class,embarked,fare,adult_f,adult_m,title ' + str(scores.mean()))\n",
    "\n",
    "#new test\n",
    "predictors = [\"Pclass\", \"Embarked\", \"Fare\", \"Adult_F\", \"Adult_M\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=10, min_samples_leaf= 5)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('6 categories: class,embarked,fare,adult_f,adult_m,title ' + str(scores.mean()))\n",
    "\n",
    "#new test\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Embarked\", \"Title\" ,\"Adult_M\",\"Adult_F\",\"Sex\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=16, min_samples_leaf= 8)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('7 categories: class,embarked,fare,adult_f,adult_m,title,sex ' + str(scores.mean()))\n",
    "\n",
    "#new test\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Embarked\", \"Title\" ,\"Adult_M\",\"Adult_F\",\"Young_F\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=16, min_samples_leaf= 8)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('6 categories: class,embarked,fare,adult_f,adult_m,title ' + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> \n",
    "From this series of test we have a noticable difference in the tree model using 6 categories: Pclass, Embarked, Fare, Adult_F, Adult_M, and title. An interesting parameter to vary is the min_samples_split and min_samples_leaf as it makes a noticable difference in the score. So we can try to maximize our best tree attempt.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,5 0.829405162738\n",
      "12,6 0.828282828283\n",
      "10,4 0.83164983165\n"
     ]
    }
   ],
   "source": [
    "#new test\n",
    "predictors = [\"Pclass\", \"Embarked\", \"Fare\", \"Adult_F\", \"Adult_M\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=10, min_samples_leaf= 5)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('10,5 ' + str(scores.mean()))\n",
    "\n",
    "#new test\n",
    "predictors = [\"Pclass\", \"Embarked\", \"Fare\", \"Adult_F\", \"Adult_M\",\"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=12, min_samples_leaf= 6)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('12,6 ' + str(scores.mean()))\n",
    "\n",
    "#new test\n",
    "predictors = [\"Pclass\", \"Embarked\", \"Fare\", \"Adult_F\", \"Adult_M\",\"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=10, min_samples_leaf= 4)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('10,4 ' + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10, 3 0.836139169473\n",
      "10, 3 0.833894500561\n"
     ]
    }
   ],
   "source": [
    "#new test\n",
    "predictors = [\"Pclass\", \"Embarked\", \"Fare\", \"Adult_F\", \"Adult_M\",\"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split= 14, min_samples_leaf= 5)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('10, 3 ' + str(scores.mean()))\n",
    "\n",
    "#new test\n",
    "predictors = [\"Pclass\", \"Embarked\", \"Fare\", \"Adult_F\", \"Adult_M\",\"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split= 17, min_samples_leaf= 4)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('10, 3 ' + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Because the top scores are close to each other we will make a submission for each and see what kaggle score we get! In addition we will submit the logistical regression model for fun to see the difference!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10, 3 0.836139169473\n",
      "10, 3 0.833894500561\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\",\"Age\", \"Sex\", \"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Title\",\"Young_M\",\"Adult_M\",\"Young_F\",\"Adult_F\"]\n",
    "\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggle_v4.csv\", index = False)\n",
    "\n",
    "#new test\n",
    "predictors = [\"Pclass\", \"Embarked\", \"Fare\", \"Adult_F\", \"Adult_M\",\"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split= 14, min_samples_leaf= 5)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('10, 3 ' + str(scores.mean()))\n",
    "\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggle_v4-1.csv\", index = False)\n",
    "\n",
    "#new test\n",
    "predictors = [\"Pclass\", \"Embarked\", \"Fare\", \"Adult_F\", \"Adult_M\",\"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split= 17, min_samples_leaf= 4)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('10, 3 ' + str(scores.mean()))\n",
    "\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggle_v4-2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "   Old Best Score: 0.78469\n",
    "   Our Improved Logistical Model Score: 0.77033\n",
    "   Our Best Random Tree Score: 0.78469\n",
    "   Our 2nd Best Random Tree Score: 0.78947\n",
    "   From our results we can see that manipulating the random tree parameters and adding new categories can help the score but probably only by a small amount unless the new parameters were something not covered by the tree before. Unfortunately Logistical Regression was not able to improve enough. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> \n",
    "Next we can try using gradient boosting classifier on our most promising category combinations to see if we can get a good score out of it </p> \n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient 6 categories: class,embarked,fare,adult_f,adult_m,title 0.82379349046\n",
      "Gradient 6 categories: class,sex,embarked,fare,adult_m,title 0.81593714927\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#new test\n",
    "predictors = [\"Pclass\", \"Embarked\", \"Fare\", \"Adult_F\", \"Adult_M\", \"Title\"]\n",
    "\n",
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=25, min_samples_split=10, min_samples_leaf= 5)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print ('Gradient 6 categories: class,embarked,fare,adult_f,adult_m,title ' + str(scores.mean()))\n",
    "\n",
    "#new test\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Embarked\", \"Title\",\"Adult_M\"]\n",
    "\n",
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=25, min_samples_split=10, min_samples_leaf= 5)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print ('Gradient 6 categories: class,sex,embarked,fare,adult_m,title ' + str(scores.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Interestingly the two best Gradient 6 categories: class,embarked,fare,adult_f,adult_m,title and Gradient 7 categories: class,sex,embarked,fare,adult_f,adult_m,title are still below the previous best tree technique. \n",
    "     \n",
    "   In an attempt to improve our Gradient Boosting Classifier which uses the errors from previous models we can try to use all of our features and more to increase the amount of weak learners and hopefully improve the algorithm.    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.361582    0.523008   \n",
      "std     257.353842    0.486592    0.836071   13.019697    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   22.000000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   35.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare     Young_M     Adult_M     Young_F     Adult_F  \n",
      "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000  \n",
      "mean     0.381594   32.204208    0.065095    0.582492    0.061728    0.290685  \n",
      "std      0.806057   49.693429    0.246833    0.493425    0.240797    0.454333  \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
      "25%      0.000000    7.910400    0.000000    0.000000    0.000000    0.000000  \n",
      "50%      0.000000   14.454200    0.000000    1.000000    0.000000    0.000000  \n",
      "75%      0.000000   31.000000    0.000000    1.000000    0.000000    1.000000  \n",
      "max      6.000000  512.329200    1.000000    1.000000    1.000000    1.000000  \n",
      "Gradient with all valid categories: 0.824915824916\n"
     ]
    }
   ],
   "source": [
    "#new test\n",
    "print titanic.describe() \n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Embarked\", \"Title\",\"Adult_M\",\"Adult_F\",\"Young_M\",\"Young_F\",\"Age\"]\n",
    "\n",
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print ('Gradient with all valid categories: ' + str(scores.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>That improved so let us try to add some more categories: \n",
    "Let's try to make sense of the cabin category, and create a new category of family size and family id to make use of the categories we weren't using\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with new valid categories: 0.822671156004\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "#method taken from datquest\n",
    "\n",
    "# Generating a familysize column based on SibSp and Parch\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "\n",
    "# A dictionary mapping family name to id\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the id given a row\n",
    "def get_family_id(row):\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family id\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the id in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum id from the mapping and add one to it if we don't have an id\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "#add family to training data \n",
    "family_ids = titanic.apply(get_family_id, axis=1)\n",
    "family_ids[titanic[\"FamilySize\"] < 3] = -1\n",
    "titanic[\"FamilyId\"] = family_ids\n",
    "\n",
    "#do the same for test_data\n",
    "family_ids = titanic_test.apply(get_family_id, axis=1)\n",
    "family_ids[titanic_test[\"FamilySize\"] < 3] = -1\n",
    "titanic_test[\"FamilyId\"] = family_ids\n",
    "\n",
    "#mapping nan and filled in values for cabin to 0.5 or 1\n",
    "titanic.loc[titanic.Cabin.isnull() == False, 'Cabin'] = 0.5\n",
    "titanic.loc[titanic.Cabin.isnull() == True, 'Cabin'] = 1\n",
    "titanic_test.loc[titanic_test.Cabin.isnull() == False, 'Cabin'] = 0.5\n",
    "titanic_test.loc[titanic_test.Cabin.isnull() == True, 'Cabin'] = 1\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Embarked\", \"Title\",\"Adult_M\",\"Adult_F\",\"Young_M\",\"Young_F\",\"Age\",\"FamilySize\",\"FamilyId\",\"Cabin\"]\n",
    "\n",
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print ('Gradient with new valid categories: ' + str(scores.mean()))\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Adding the two categories improved the model above some but made it worse than the gradient with 5. This is probably due to misleading predictors messing the algorithm up. Next we can try taking out all the categories that are based off each other and seeing if that improves it and if so make a submission!\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: 0.824915824916\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Fare\", \"Embarked\", \"Title\",\"Adult_M\",\"Adult_F\",\"FamilySize\",\"FamilyId\"]\n",
    "\n",
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print ('Gradient: ' + str(scores.mean()))\n",
    "\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggle_v5-1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "        Wooo the gradient model improved the model(Kaggle: 0.79904) - despite having a significantly worse score than the tree versions which seems to indicate the other models were overfitting. We can try to use these new categories in our Rtrees and logistical to improve the score.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 categories old 0.818181818182\n",
      "0.812570145903\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Embarked\", \"Title\",\"FamilySize\",\"FamilyId\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=150, min_samples_split=10, min_samples_leaf= 5)\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv =3)\n",
    "\n",
    "print('5 categories old ' + str(scores.mean()))\n",
    "\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggle_v5-3.csv\", index = False)\n",
    "\n",
    "predictors = [\"Pclass\",\"Age\", \"Sex\", \"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Title\",\"Young_M\",\"Adult_M\",\"Young_F\",\"Adult_F\",\"FamilySize\",\"FamilyId\"]\n",
    "# Initialize the algorithm class\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set.\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "print scores.mean()\n",
    "\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggle_v5-4.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The previous combination creates a Kaggle Score of 0.78469 which was still worse than our best rtree model which seems to indicate that the categories are not enough. Adding the new categories improved the logistical regression score to : 0.7703.\n",
    "Now we can combine the best gradient boosting with best logistic regression and the best tree with best logistic regression to see if our score improves.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wilson/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:35: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n",
      "/home/wilson/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:81: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.820426487093\n",
      "0.819304152637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "# The algorithms we want to ensemble.\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier.\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Fare\", \"Embarked\", \"Title\",\"Adult_M\",\"Adult_F\",\"FamilySize\",\"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\",\"Age\", \"Sex\", \"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"Title\",\"FamilySize\",\"FamilyId\",\"Young_M\",\"Adult_M\",\"Young_F\",\"Adult_F\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)\n",
    "\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggle_v6-1.csv\", index = False)\n",
    "\n",
    "#Random Trees and Logistical Regression\n",
    "algorithms = [\n",
    "    [RandomForestClassifier(random_state=1, n_estimators=25, min_samples_split=10, min_samples_leaf= 5), [\"Pclass\", \"Embarked\", \"Fare\", \"Adult_F\", \"Adult_M\", \"Title\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\",\"Age\", \"Sex\", \"SibSp\",\"Parch\",\"Fare\",\"Embarked\",\"FamilySize\",\"FamilyId\",\"Title\",\"Young_M\",\"Adult_M\",\"Young_F\",\"Adult_F\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data.\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold.  \n",
    "        # The .astype(float) is necessary to convert the dataframe to all floats and avoid an sklearn error.\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme -- just average the predictions to get the final classification.\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction.\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array.\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data.\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)\n",
    "\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv(\"kaggle_v6-2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "        Unfortunately the difference in models is too much between the tree and logistical to benefit from the combination. The accuracy is very low for both combinations compared to the tree alone and even variations in the set will probably not make up the difference. But we can try the second model out to see what kaggle score we get.\n",
    "        Kaggle for Gradient and Logistical: 0.773 , which went down probably due to the logistical regression model being too low\n",
    "        Kaggle for Tree and Logistical: 0.77 which also went down probably due to the logistical regression model being too low\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    In ending notes we see that the different models all have different results due to the available information and that adding parameters/features only works if the features are meaningful. In addition the ability of gradient was able to use the weak models benefitted from using the weaker data and features. Unfortunately we weren't able to improve the logistical regression model enough to have it assists the gradient algorithm. Final best score: 0.79904\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
